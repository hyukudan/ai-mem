Metadata-Version: 2.4
Name: ai-mem
Version: 0.1.0
Summary: Long-term memory for LLMs
Author-email: Your Name <your.email@example.com>
License: AGPL-3.0
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typer>=0.9.0
Requires-Dist: rich>=13.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: chromadb>=0.4.0
Requires-Dist: google-generativeai>=0.3.0
Requires-Dist: openai>=1.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: fastapi>=0.100.0
Requires-Dist: uvicorn>=0.20.0
Requires-Dist: pathspec>=0.11.0
Requires-Dist: fastembed>=0.4.0
Dynamic: license-file

# ai-mem: Universal Long-Term Memory for LLMs

**ai-mem** is an open-source tool designed to give **Long-Term Memory** to any Large Language Model (LLM), including Google Gemini, OpenAI-compatible local models (vLLM/LM Studio/Ollama), and more.

Inspired by [claude-mem](https://github.com/thedotmack/claude-mem), this project generalizes the concept of persistent context, allowing you to maintain knowledge across sessions, projects, and different AI providers.

![License](https://img.shields.io/badge/license-AGPL%203.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.10+-green.svg)

## üöÄ Why ai-mem?

Current LLM interfaces have a limited context window. Once you close a chat or start a new session, the model "forgets" everything you taught it about your coding style, project architecture, or specific requirements.

**ai-mem** solves this by acting as an external **Memory Layer** that:
1.  **Captures** important interactions, facts, and decisions.
2.  **Compresses** verbose logs into concise insights using an AI summarizer.
3.  **Stores** knowledge in a local Vector Database (ChromaDB) with project-level isolation.
4.  **Retrieves** the most relevant context automatically via Semantic Search (RAG) when you start a new task.

## ‚ú® Key Features

*   **ü§ñ Model Agnostic:** Gemini native + OpenAI-compatible local models out of the box.
*   **üîí Local & Private:** Your data stays on your machine (SQLite + FTS5 + ChromaDB).
*   **üß© Local Embeddings:** Fast, offline embeddings via `fastembed` by default.
*   **üìÇ Project Scoped:** Automatically detects your current project directory to keep memories organized.
*   **üï∏Ô∏è Web Viewer UI:** A built-in dashboard to visualize, search, and manage your memory stream in real-time.
*   **üîå API Server:** Exposes a REST API so other agents, scripts, or IDE extensions can query memory programmatically.
*   **üß† Semantic Search:** Finds information by *meaning*, not just keywords.
*   **üìâ Cost & Token Management:** Summarizes old memories to save on context window usage.

## üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ai-mem.git
cd ai-mem

# Install dependencies (global)
pip install -e .
```

## üß™ Virtualenv (Recommended)

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -e .
```

Or use the bootstrap script:

```bash
./scripts/bootstrap.sh
```

## ‚ö° Quick Start

### 1. Zero-config (CLI only)
By default, `ai-mem` runs with **local embeddings** and **no LLM** required. This lets you use the CLI without API keys.

```bash
ai-mem add "We use Python 3.11 and pandas 2.0 in Omega."
ai-mem search "Omega dependencies"
```

### 2. Configuration (Optional)
Configure an LLM provider and embeddings. Embeddings are local (`fastembed`) unless you change them.

```bash
# Gemini (LLM) + local embeddings
ai-mem config --llm-provider gemini --llm-model gemini-1.5-flash --llm-api-key YOUR_GEMINI_API_KEY
ai-mem config --embeddings-provider fastembed

# Local OpenAI-compatible LLM (vLLM/LM Studio/Ollama)
ai-mem config --llm-provider openai-compatible --llm-base-url http://localhost:8000/v1 --llm-model YOUR_MODEL_NAME
```

### 3. Adding Memories
You can manually add knowledge or pipe output from other commands.

```bash
# Manual addition
ai-mem add "The project 'Omega' uses Python 3.11 and requires pandas 2.0. Avoid using numpy directly."

# Ingest a codebase
ai-mem ingest .
```

### 4. Retrieving Context
Before asking your LLM a question, fetch the relevant context.

```bash
# Compact index (Layer 1)
ai-mem search "How do I set up the dependencies for Omega?"

# Timeline context (Layer 2)
ai-mem timeline --query "dependencies for Omega"

# Full details (Layer 3)
ai-mem get <observation_id>
```

### 4. The Web Viewer
Launch the local server to browse your memories visually.

```bash
ai-mem server
```
> Open `http://localhost:8000` in your browser to see the Memory Stream.

## üõ†Ô∏è Architecture

**ai-mem** follows a layered retrieval pattern:

1.  **SQLite + FTS5:** Fast keyword search for observations, summaries, and tags.
2.  **ChromaDB:** Semantic retrieval via local embeddings.
3.  **Progressive Disclosure:** `search` ‚Üí `timeline` ‚Üí `get` keeps token usage low.

## ü§ñ Automation & Integration

### Using with Scripts
You can use `ai-mem` in your CI/CD pipelines or local scripts to inject context automatically.

```bash
# Example: Injecting memory into a CLI LLM tool
CONTEXT=$(ai-mem get "Fixing the login bug" --format json)
llm-tool --system "$CONTEXT" "Help me fix the login bug in auth.py"
```

### REST API
When you run `ai-mem server`, a full REST API is available at `http://localhost:8000/docs`.

*   `POST /api/memories`: Add a new memory.
*   `GET /api/search?query=...`: Search the memory index.
*   `GET /api/timeline?query=...`: Timeline context around a query.
*   `POST /api/observations`: Fetch full observation details.
*   `GET /api/projects`: List all tracked projects.

## üó∫Ô∏è Roadmap

- [x] Core CLI (Config, Add, Search, Timeline, Get)
- [x] Gemini Provider Support
- [x] Local embeddings (fastembed)
- [ ] OpenAI-compatible embeddings auto-detect
- [ ] VS Code Extension
- [ ] "Watch Mode" (Auto-ingest terminal output)

## üôå Attribution

This project is inspired by and references architecture from [claude-mem](https://github.com/thedotmack/claude-mem). Portions of the design and concepts follow claude-mem's progressive disclosure and storage model. See their repo for the original implementation.

## üìÑ License

AGPL-3.0 License.
