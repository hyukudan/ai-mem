# ai-mem Environment Variables Example
# Copy to .env and customize for your setup
# All variables are optional - sensible defaults are used

# =============================================================================
# LLM Provider (for summarization)
# =============================================================================
# AI_MEM_LLM_PROVIDER=gemini  # Options: gemini, anthropic, openai, azure-openai, bedrock, none
# AI_MEM_LLM_MODEL=gemini-1.5-flash
# AI_MEM_LLM_API_KEY=your-api-key-here
# AI_MEM_LLM_BASE_URL=https://api.example.com/v1  # For OpenAI-compatible endpoints

# =============================================================================
# Embeddings Provider
# =============================================================================
# AI_MEM_EMBEDDINGS_PROVIDER=fastembed  # Options: fastembed, openai, azure-openai, bedrock, gemini
# AI_MEM_EMBEDDINGS_MODEL=BAAI/bge-small-en-v1.5
# AI_MEM_EMBEDDINGS_API_KEY=your-api-key-here

# =============================================================================
# Storage
# =============================================================================
# AI_MEM_DATA_DIR=~/.ai-mem
# AI_MEM_CONFIG=~/.config/ai-mem/config.json

# =============================================================================
# Vector Store
# =============================================================================
# AI_MEM_VECTOR_PROVIDER=chroma  # Options: chroma, pgvector, qdrant
# AI_MEM_VECTOR_CHROMA_COLLECTION=observations

# PGVector (if using pgvector)
# AI_MEM_PGVECTOR_DSN=postgresql://user:pass@localhost:5432/ai_mem
# AI_MEM_PGVECTOR_TABLE=ai_mem_vectors
# AI_MEM_PGVECTOR_DIMENSION=1536

# Qdrant (if using qdrant)
# AI_MEM_QDRANT_URL=http://localhost:6333
# AI_MEM_QDRANT_API_KEY=your-qdrant-key
# AI_MEM_QDRANT_COLLECTION=observations

# =============================================================================
# Search & Caching
# =============================================================================
# AI_MEM_SEARCH_CACHE_TTL=30
# AI_MEM_SEARCH_CACHE_ENTRIES=256
# AI_MEM_SEARCH_FTS_WEIGHT=0.5
# AI_MEM_SEARCH_VECTOR_WEIGHT=0.5
# AI_MEM_SEARCH_RECENCY_HALFLIFE_HOURS=48
# AI_MEM_SEARCH_RECENCY_WEIGHT=0.1

# =============================================================================
# Context Injection
# =============================================================================
# AI_MEM_CONTEXT_TOTAL=12       # Total observations in index
# AI_MEM_CONTEXT_FULL=4         # Observations with full content
# AI_MEM_CONTEXT_TYPES=note,bugfix,decision  # Filter by types
# AI_MEM_CONTEXT_TAGS=important,auth         # Filter by tags
# AI_MEM_CONTEXT_FULL_FIELD=content          # 'content' or 'summary'
# AI_MEM_CONTEXT_SHOW_TOKENS=true
# AI_MEM_CONTEXT_WRAP=true

# =============================================================================
# Ingestion Filtering (NEW - noise control & privacy)
# =============================================================================
# Tools to skip entirely (comma-separated, exact match)
# Default: SlashCommand,Skill,TodoWrite,TodoRead,AskFollowupQuestion,AttemptCompletion
# AI_MEM_SKIP_TOOL_NAMES=SlashCommand,Skill,TodoWrite,TodoRead

# Tool name prefixes to skip (e.g., mcp__ for all MCP tools)
# AI_MEM_SKIP_TOOL_PREFIXES=mcp__,_internal

# Tool categories to skip (if host provides category info)
# AI_MEM_SKIP_TOOL_CATEGORIES=meta,admin

# Content truncation limits
# AI_MEM_MAX_OUTPUT_CHARS=50000  # Max tool output chars (default 50000)
# AI_MEM_MAX_INPUT_CHARS=10000   # Max tool input chars (default 10000)
# AI_MEM_MIN_OUTPUT_CHARS=0      # Skip tools with output shorter than this

# Skip tools that failed
# AI_MEM_IGNORE_FAILED_TOOLS=false

# Default tags for auto-ingested observations
# AI_MEM_INGESTION_DEFAULT_TAGS=tool,auto-ingested

# =============================================================================
# Host Identification (LLM-agnostic)
# =============================================================================
# Identifies which host/agent generated the events.
# Each terminal/process can have its own host - the memory aggregates all.
# This enables multi-LLM workflows where Claude + Gemini + Cursor all
# write to the same shared memory.
#
# Example setups:
#   Terminal 1 (Claude): AI_MEM_HOST=claude-code
#   Terminal 2 (Gemini): AI_MEM_HOST=gemini-cli
#   Terminal 3 (Cursor): AI_MEM_HOST=cursor
#
# AI_MEM_HOST=claude-code  # Options: claude-code, gemini, gemini-cli, cursor, vscode, custom

# =============================================================================
# API Security (Optional)
# =============================================================================
# AI_MEM_API_TOKEN=your-secret-token  # Require bearer token for all API routes

# =============================================================================
# Session Tracking
# =============================================================================
# AI_MEM_SESSION_TRACKING=true      # Auto-start sessions on hook events
# AI_MEM_SESSION_ID=                 # Override session ID (usually auto-generated)
# AI_MEM_PROJECT=                    # Override project path
# AI_MEM_SUMMARY_ON_END=true        # Generate summary when session ends
# AI_MEM_SUMMARY_COUNT=20           # Number of observations to include in summary

# =============================================================================
# Bedrock (AWS)
# =============================================================================
# AI_MEM_BEDROCK_REGION=us-east-1
# AI_MEM_BEDROCK_ENDPOINT=
# AI_MEM_BEDROCK_PROFILE=default
# AI_MEM_BEDROCK_MAX_TOKENS=1024

# =============================================================================
# Azure OpenAI
# =============================================================================
# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AI_MEM_AZURE_API_VERSION=2024-02-01
