# ai-mem: Universal Long-Term Memory for LLMs

![ai-mem banner](assets/banner.png)
![License](https://img.shields.io/badge/license-AGPL--3.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-blue)
![Status](https://img.shields.io/badge/status-active-success)

 ai-mem is a local-first memory layer that serves **any large language model**, whether it's Gemini (CLI/proxy), Claude, ChatGPT, Anthropic, Azure OpenAI, AWS Bedrock, or another vLLM setup. Observations live in SQLite (with FTS5) + a vector store, memory context is generated on demand, and the same context feeds every client through CLI, REST, or the MCP protocol.

## Why ai-mem matters

- **Open memory space for every model** ‚Äì Claude, Gemini, ChatGPT, and other assistants consume the same context stream, so a discovery you capture from one model appears automatically in all other agents that query the shared store.
- **Transparent relevance insights** ‚Äì Each context exposure returns a token scoreboard + cache health details, making it easy to see why memories were selected and to tune injection windows without opening the UI.
- **Private, local-first runtime** ‚Äì All data is stored on disk via SQLite/FTS5 and Chroma (vector store), so you can run without cloud APIs and keep everything inside your workstation.
- **Composable integrations** ‚Äì Includes hooks for IDEs (VS Code, JetBrains, Antigravity, Claude Desktop, etc.), MCP clients, shell scripts, and CLI helpers, so no manual instrumentation is required.
- **Persistent, adaptive memory** ‚Äì ‚ÄúEndless Mode‚Äù auto-refreshes context, while `snapshot merge` lets you surface long-lived checkpoints across tasks.

## Getting started (local-first)

1. **Bootstrap the environment**

   ```bash
   ./scripts/bootstrap.sh
   source .venv/bin/activate
   ```

   This script creates a Python virtualenv, pins tooling, and installs `ai-mem` in editable mode.

2. **Capture memory**

   ```bash
   ai-mem add "We use Python 3.11 and pandas 2.0"
   ai-mem search "Python dependencies"
   ```

3. **Share context across models**

   - `ai-mem context ...` formats context with `<ai-mem-context>` wrappers for easy injection into Claude Desktop, Gemini CLI, or other assistants.
   - `ai-mem timeline` provides progressive disclosure (search ‚Üí timeline ‚Üí full detail).
   - `ai-mem endless` keeps regenerating context, prints token totals/scoreboard, and adapts the window automatically to stay within your token budget.

4. **Persist checkpoints**

   ```bash
   ai-mem snapshot export path/to/snapshot.ndjson
   ai-mem snapshot merge <checkpoint-id>
   ```

   Snapshots dump observations so you can sync across sessions or share with other team members. `snapshot merge` imports an existing checkpoint into the local store while keeping provenance metadata.

5. **Launch the UI**

   ```bash
   ./scripts/run.sh        # starts the web viewer and MCP server at http://localhost:37777
   ./scripts/run-gemini-full.sh  # proxy + Gemini-native integration
   ```

   The UI streams live observations, shows cache health, exposes scoreboard metadata, and provides links to `/api/observation/{id}` for citations.

## Featured capabilities

- **Model-agnostic context streaming** ‚Äì Context chunks are formatted via the same helper no matter which model requests them, so your Claude Desktop session, Gemini CLI, or ChatGPT plugin can all read from a single source of truth.
- **Shared metadata & scoreboard** ‚Äì Responses include metadata that enumerates vector vs. FTS scores, recency, and cache hits/misses, letting you introspect model prompts before each completion.
- **Endless Mode** ‚Äì `ai-mem endless` polls the store at a configurable interval to keep total tokens within limits. See [Endless Mode Guide](docs/endless_mode.md).
- **Snapshot-based syncing** ‚Äì `ai-mem snapshot export`/`import`/`merge` round-trips checkpoints. See [Snapshots Guide](docs/snapshots.md).
- **Hooks & IDE scripts** ‚Äì Run `./scripts/install-hooks.sh`, `./scripts/install-vscode-tasks.sh`, `./scripts/install-jetbrains-tools.sh`, or the Antigravity/Claude installers to make every hook call `ai-mem hook ...` automatically.
- **Web viewer + MCP server** ‚Äì Live stream + scoreboard at `http://localhost:37777`, plus REST endpoints that expose observations, scorecards, and citations for external agents like Gemini, Claude, or Antigravity.
- **CLI-first control** ‚Äì All key flows (add/search/context/timeline/endless/snapshot) are available via `ai-mem` so you can script onboarding via a single CLI.
- **Scripts library** ‚Äì `./scripts/run*.sh` cover full/stack/proxy setups for Gemini, Claude, Bedrock, Azure, and Anthropic deployments, ensuring the UI + MCP server is always upstream.

## Cross-model handoff

1. Start the server/panel stack so the MCP tools, REST API, and UI feed a shared memory graph:

   ```bash
   ./scripts/run-all.sh
   ```

2. Keep a live stream via `ai-mem endless` to continuously refresh context; the same `scoreboard` and cache entries that appear in the UI are printed on every iteration so assistants can explain why memories were selected:

   ```bash
   ai-mem endless --query "next feature" --interval 30 --token-limit 1000
   ```

3. Point Claude Desktop, Gemini CLI, Antigravity, or any custom vLLM at the MCP endpoint‚Äîeach client consumes the same `<ai-mem-context>` blocks, scoreboard, and citations so your investigations stay synchronized.

4. Export and merge checkpoints when you want to hand off context between machines or sessions:

   ```bash
   ai-mem snapshot export /tmp/ai-mem-checkpoint.ndjson
   ai-mem snapshot merge /tmp/ai-mem-checkpoint.ndjson
   ```

This workflow keeps Claude, Gemini, and other assistants aligned with the same persistent history, token budgets, and metadata traceability across every interaction.

## Documentation

See the `docs/` folder for targeted guides:

- [üöÄ Getting Started](docs/getting-started.md) ‚Äì Installation, configuration, quick-start, and baseline workflows.
- [‚öôÔ∏è Configuration](docs/configuration.md) ‚Äì Vector stores, cache policies, connectors, and token budget controls.
- [üîå Proxies & Hooks](docs/proxies.md) ‚Äì OpenAI, Gemini, Anthropic, Azure, Bedrock proxies plus IDE integrations.
- [üõ†Ô∏è MCP Tools & Integrations](docs/mcp-tools.md) ‚Äì MCP Server, Claude plugins, Antigravity, VS Code, cursor hooks, and how to consume context over MCP.
- [üé£ Hooks](docs/hooks.md) ‚Äì Lifecycle hooks for command-line, IDE, and MCP-based ingestion.
- [üß© Presets](docs/presets.md) ‚Äì Scripts for installing Claude mem-search skills, VS Code tasks, JetBrains tools, and Antigravity/VS integrations so everything wires into `ai-mem hook ...`.
- [üìñ API Reference](docs/api-reference.md) ‚Äì REST routes, observation schema, scoreboard payloads.
- [üèõÔ∏è Architecture](docs/architecture.md) ‚Äì Component diagrams, data flow, and search strategy.
- [‚àû Endless Mode](docs/endless_mode.md) ‚Äì Continuous context injection and scoreboard monitoring.
- [üì∏ Snapshots](docs/snapshots.md) ‚Äì Backup, export, and merging strategies.
- [üíª Development](docs/development.md) ‚Äì Testing, roadmap, contributor guide, and venv tips.

## Credits & inspiration

Inspired by [claude-mem](https://github.com/thedotmack/claude-mem). We highlight the shared-memory story, CLI helpers, and documentation so any LLM can plug in and tap into persistent context.
